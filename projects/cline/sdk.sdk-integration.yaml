# SDK Integration Analysis: Cline
# Spec: specs/sdk-integration.spec.yaml

project: "cline"
sdk_integration_detected: true

summary: |
  Direct Anthropic SDK integration via @anthropic-ai/sdk v0.37.0 with full streaming support,
  prompt caching (ephemeral cache control on last 2 user messages), extended thinking with
  budget tokens, native tool calling, and 1M context window support via beta headers.

sdks_used:
  - name: "anthropic-typescript"
    package: "@anthropic-ai/sdk"
    version_constraint: "^0.37.0"
    import_reference:
      path: "src/core/api/providers/anthropic.ts"
      lines: [1, 3]
      snippet: |
        import { Anthropic } from "@anthropic-ai/sdk"
        import { Tool as AnthropicTool } from "@anthropic-ai/sdk/resources/index"
        import { Stream as AnthropicStream } from "@anthropic-ai/sdk/streaming"

  - name: "anthropic-vertex"
    package: "@anthropic-ai/vertex-sdk"
    version_constraint: "^0.6.4"
    import_reference:
      path: "package.json"
      lines: [504, 504]
      snippet: |
        "@anthropic-ai/vertex-sdk": "^0.6.4",

sdk_usage:

  - id: streaming-messages-create
    sdk: "anthropic-typescript"
    pattern: "messages-create-stream"
    description: "Primary streaming message creation with prompt caching and extended thinking"

    reference:
      repository: "https://github.com/cline/cline"
      commit: "844038084c6e559ee131c32e5d5da93dcff68a73"
      path: "src/core/api/providers/anthropic.ts"
      lines: [66, 116]
      function: "createMessage"
      class: "AnthropicHandler"
      language: "typescript"

    api_methods:
      - "client.messages.create"

    parameters:
      - name: "model"
        value_type: "string"
        purpose: "Model selection"
        example_value: "claude-sonnet-4-5-20250929"
      - name: "thinking"
        value_type: "object"
        purpose: "Extended thinking with budget tokens"
        example_value: "{ type: 'enabled', budget_tokens: 6000 }"
      - name: "max_tokens"
        value_type: "integer"
        purpose: "Response length limit"
        example_value: "8192"
      - name: "temperature"
        value_type: "float"
        purpose: "Sampling temperature (disabled when thinking enabled)"
        example_value: "0"
      - name: "stream"
        value_type: "boolean"
        purpose: "Enable streaming"
        example_value: "true"
      - name: "tools"
        value_type: "array"
        purpose: "Native tool calling definitions"
      - name: "tool_choice"
        value_type: "object"
        purpose: "Tool selection strategy"
        example_value: "{ type: 'any' }"

    snippet: |
      stream = await client.messages.create(
        {
          model: modelId,
          thinking: reasoningOn ? { type: "enabled", budget_tokens: budget_tokens } : undefined,
          max_tokens: model.info.maxTokens || 8192,
          temperature: reasoningOn ? undefined : 0,
          system: [
            {
              text: systemPrompt,
              type: "text",
              cache_control: { type: "ephemeral" },
            },
          ],
          messages: anthropicMessages,
          stream: true,
          tools: nativeToolsOn ? tools : undefined,
          tool_choice: nativeToolsOn && !reasoningOn ? { type: "any" } : undefined,
        },
        (() => {
          // 1m context window beta header
          if (enable1mContextWindow) {
            return {
              headers: {
                "anthropic-beta": "context-1m-2025-08-07",
              },
            }
          } else {
            return undefined
          }
        })(),
      )

    notes: |
      Uses prompt caching (ephemeral) on system message and last 2 user messages.
      Extended thinking enabled when model supports it and budget > 0 (min 1024, max 6000).
      1M context window via beta header. Native tool calling with streaming.

    checklist_refs:
      - "sdk-features.checklist.yaml#messages-stream"
      - "sdk-features.checklist.yaml#tool-use"

  - id: stream-event-processing
    sdk: "anthropic-typescript"
    pattern: "stream-events"
    description: "Processes streaming events including thinking, text, and tool_use blocks"

    reference:
      repository: "https://github.com/cline/cline"
      commit: "844038084c6e559ee131c32e5d5da93dcff68a73"
      path: "src/core/api/providers/anthropic.ts"
      lines: [120, 239]
      function: "createMessage"
      class: "AnthropicHandler"
      language: "typescript"

    snippet: |
      for await (const chunk of stream) {
        switch (chunk?.type) {
          case "message_start":
            {
              const usage = chunk.message.usage
              yield {
                type: "usage",
                inputTokens: usage.input_tokens || 0,
                outputTokens: usage.output_tokens || 0,
                cacheWriteTokens: usage.cache_creation_input_tokens || undefined,
                cacheReadTokens: usage.cache_read_input_tokens || undefined,
              }
            }
            break
          case "content_block_start":
            switch (chunk.content_block.type) {
              case "thinking":
                yield {
                  type: "reasoning",
                  reasoning: chunk.content_block.thinking || "",
                  signature: chunk.content_block.signature,
                }
                break
              case "redacted_thinking":
                yield {
                  type: "reasoning",
                  reasoning: "[Redacted thinking block]",
                  redacted_data: chunk.content_block.data,
                }
                break
              case "tool_use":
                if (chunk.content_block.id && chunk.content_block.name) {
                  lastStartedToolCall.id = chunk.content_block.id
                  lastStartedToolCall.name = chunk.content_block.name
                  lastStartedToolCall.arguments = ""
                }
                break
              case "text":
                if (chunk.index > 0) {
                  yield { type: "text", text: "\n" }
                }
                yield { type: "text", text: chunk.content_block.text }
                break
            }
            break
          case "content_block_delta":
            switch (chunk.delta.type) {
              case "thinking_delta":
                yield { type: "reasoning", reasoning: chunk.delta.thinking }
                break
              case "signature_delta":
                if (chunk.delta.signature) {
                  yield { type: "reasoning", reasoning: "", signature: chunk.delta.signature }
                }
                break
              case "text_delta":
                yield { type: "text", text: chunk.delta.text }
                break
              case "input_json_delta":
                // Convert Anthropic tool_use to OpenAI-compatible format
                if (lastStartedToolCall.id && lastStartedToolCall.name && chunk.delta.partial_json) {
                  yield { type: "tool_calls", tool_call: { ...lastStartedToolCall, function: { ...lastStartedToolCall, arguments: chunk.delta.partial_json } } }
                }
                break
            }
            break
          case "content_block_stop":
            lastStartedToolCall.id = ""
            lastStartedToolCall.name = ""
            lastStartedToolCall.arguments = ""
            break
        }
      }

    notes: |
      Handles message_start (usage tracking with cache read/write tokens),
      content_block_start (thinking, redacted_thinking, tool_use, text),
      content_block_delta (thinking_delta, signature_delta, text_delta, input_json_delta),
      and content_block_stop (reset tool call state).

agents_sdk_features: {}

patterns_summary:
  messages-stream: true
  tool-use: true
  prompt-caching: true
  extended-thinking: true
  agent-create: false
  1m-context-window: true
