# SDK Integration Analysis: AlphaCodium
# Spec: specs/sdk-integration.spec.yaml

project: "alphacodium"
sdk_integration_detected: true

summary: |
  Uses LiteLLM as abstraction layer for LLM provider access. No direct Anthropic SDK
  import or Claude CLI invocation. Claude models supported through LiteLLM's model routing
  (model name prefix determines provider). Default model is GPT-4; Claude usable by
  configuring model="claude-3-opus" in configuration.toml. Features async completion,
  model fallback chains, and rate limiting.

sdks_used:
  - name: "litellm"
    package: "litellm"
    version_constraint: "*"
    import_reference:
      path: "alpha_codium/llm/ai_handler.py"
      lines: [4, 7]
      snippet: |
        import litellm
        import openai
        from aiolimiter import AsyncLimiter
        from litellm import acompletion

sdk_usage:

  - id: litellm-acompletion
    sdk: "litellm"
    pattern: "messages-create"
    description: "Async chat completion via LiteLLM acompletion() with model-agnostic routing"

    reference:
      repository: "https://github.com/Codium-ai/AlphaCodium"
      commit: "eb7577dbe998ae7e55696264591ac3c5dde75638"
      path: "alpha_codium/llm/ai_handler.py"
      lines: [70, 135]
      function: "chat_completion"
      class: "AiHandler"
      language: "python"

    api_methods:
      - "litellm.acompletion"

    parameters:
      - name: "model"
        value_type: "string"
        purpose: "Model selection (routes to provider based on prefix)"
        example_value: "gpt-4-0125-preview"
      - name: "messages"
        value_type: "list"
        purpose: "System + user message pair"
      - name: "temperature"
        value_type: "float"
        purpose: "Sampling temperature (default 0.2)"
      - name: "frequency_penalty"
        value_type: "float"
        purpose: "Frequency penalty"
      - name: "force_timeout"
        value_type: "integer"
        purpose: "API timeout from config (default 90s)"

    snippet: |
      response = await acompletion(
          model=model,
          deployment_id=deployment_id,
          messages=[
              {"role": "system", "content": system},
              {"role": "user", "content": user},
          ],
          temperature=temperature,
          frequency_penalty=frequency_penalty,
          force_timeout=get_settings().config.ai_timeout,
      )

    notes: |
      Model-agnostic: Claude models work by setting model="claude-3-opus" etc.
      Configuration via Dynaconf TOML files. Default model is GPT-4.

  - id: model-fallback
    sdk: "litellm"
    pattern: "model-fallback"
    description: "Automatic model fallback chain on failure"

    reference:
      repository: "https://github.com/Codium-ai/AlphaCodium"
      commit: "eb7577dbe998ae7e55696264591ac3c5dde75638"
      path: "alpha_codium/llm/ai_invoker.py"
      lines: [8, 23]
      function: "send_inference"
      language: "python"

    snippet: |
      async def send_inference(f: Callable):
          all_models = _get_all_models()
          all_deployments = _get_all_deployments(all_models)
          # try each (model, deployment_id) pair until one is successful, otherwise raise exception
          for i, (model, deployment_id) in enumerate(zip(all_models, all_deployments)):
              try:
                  get_settings().set("openai.deployment_id", deployment_id)
                  return await f(model)
              except Exception:
                  logging.warning(
                      f"Failed to generate prediction with {model}"
                      f"{(' from deployment ' + deployment_id) if deployment_id else ''}: "
                      f"{traceback.format_exc()}"
                  )
                  if i == len(all_models) - 1:  # If it's the last iteration
                      raise  # Re-raise the last exception

    notes: |
      Supports fallback_models config for cascading through multiple models.
      Could cascade from Claude to GPT or vice versa.

agents_sdk_features: {}

patterns_summary:
  messages-create: true
  messages-stream: false
  tool-use: false
  prompt-caching: false
  extended-thinking: false
  vision: false
  model-fallback: true
  agent-create: false
